# -*- coding: utf-8 -*-
"""House_Prices.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xrri_kkhEimmjjOAGT7tike6dm_eeaNm

#House Prices - Advanced Regression Techniques
##Predict sales prices and practice feature engineering, RFs, and gradient boosting

import libraries and data loading from Kaggle "house-prices-advanced-regression-techniques"
and loading data
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import warnings
warnings.simplefilter("ignore")

from google.colab import drive
drive.mount('/content/gdrive')
# main_path = '/content/gdrive/Othercomputers/ה-מחשב נייד שלי/תואר שני/הסקה סטטיסטית וכריית נתונים/Assignments/HW2/Kaggle - House Prices/house-prices-advanced-regression-techniques'
main_path = '/content/gdrive/MyDrive/Statistical_Data_Mining/Assignment_2/house-prices-advanced-regression-techniques'

train = pd.read_csv(main_path + '/train.csv')
test = pd.read_csv(main_path + '/test.csv')
submit = pd.read_csv(main_path + "/sample_submission.csv")

"""# Section 1: Database Overview and Visualization

In this section we will overview the database, understand it structure.

## 1.1 Data discription

We will make new database 'house_df' that is the concatinate of the test and train so wi can remove or add features to all the frames conveniently
"""

print("train database:")
train

print("test database:")
test

house_df = pd.concat([train,test],ignore_index = True, sort = False)
tr_idx = house_df['SalePrice'].notnull() ## display data with on
te_idx = [not elem for elem in tr_idx]
te_idx = pd.Series(te_idx)
print("house dataframe shape:{}".format(house_df.shape))

train

house_df[tr_idx]

"""## 1.2 Missing featurs

There are X fearute which dosent comprised from all samples in the dataframe:
* LotFrontage
* Alley
* BsmtQual
* MasVnrArea
* MasVnrArea
* BsmtCond
* BsmtExposure
* BsmtFinType1
* BsmtFinType2
* Electrical
* FireplaceQu     
* GarageType                                                           
* GarageCond       
* PoolQC         
* Fence          
* MiscFeature

Train and Test missing data:
"""

# house_df missing features
print("house_df missing features")
print("house_df shape: {}".format(train.shape))
pd.set_option('display.max_rows', None)
missing_data = pd.DataFrame(house_df.isna().sum())
missing_data_prec = missing_data/house_df.shape[0] * 100
missing_data = pd.concat([missing_data,missing_data_prec], axis=1)
missing_data.columns =['occurence', 'precentage']
print(missing_data)
pd.reset_option('all')

"""Train missing data:"""

# train missing features
print('train missing features')
print('train shape: {}'.format(train.shape[0]))
pd.set_option('display.max_rows', None)
missing_data = pd.DataFrame(train.isna().sum())
missing_data_prec = missing_data/train.shape[0] * 100
missing_data = pd.concat([missing_data,missing_data_prec], axis=1)
missing_data.columns =['occurence', 'precentage']
print(missing_data)
pd.reset_option('all')

"""***Most missing features are:***
1.   "Alley" with 93% of missing values
2.   "PoolQC" with 99.52% of missing values
3.   "Fence" with 80.75% of missing values
4.   "MiscFeature" with 96.3% of missing values

## 1.3 Features type distribution

There are 81 diffrent feature, that is a lot of data to handle and process. Additionally, the feature are in diffrent type, not all of them are numerical, they can be splited according to type: 
1. int - 43 features
2. float64 - 25 features
3. object - 12 features
"""

plt.figure(figsize = (8,6))
ax = house_df.dtypes.value_counts().plot(kind='bar',grid = False,fontsize=20,color='grey')
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x()+ p.get_width() / 2., height + 1, height, ha = 'center', size = 25)
sns.despine()

"""Statistical description of the numerical features"""

# Amit: I think this part irellevent duo to lack of normalization step. It would be better if we normalize and reference this subsection 
house_df.describe().T.style.set_properties(**{'background-color': 'Grey',
                           'color': 'white',
                           'border-color': 'darkblack'})

"""## 1.4 Feature Histogram

Spliting the data according to the type from the previous subsection and ploting hitsograme for every feature to further understanding the numerical meaning behind the features.
It can be conclude from the histogram that:

1. **float64 features:**

  Three float64 features are in fact discrete variables:
  * BsmtHalfBath - Basement half bathrooms
  * BsmtFullBath - Basement full bathrooms
  * GarageCars - Size of garage in car capacity
  
  Some features have a skewed shape to one side.


2. **int features:**

  Five int features values does not represent quantitative sizes
  * MSSubClass - Identifies the type of dwelling involved in the sale.
  * YearBuilt - Original construction date
  * YearRemodAdd - Remodel date (same as construction date if no remodeling or additions)
  * MoSold - Month Sold (MM)
  * YrSold - Year Sold (YYYY)


"""

# Select categorical columns
categorical_cols = [cname for cname in house_df.loc[:,:'SaleCondition'].columns if
                    house_df[cname].nunique() < 200 and 
                    house_df[cname].dtype == "object"]

print("Number of Categorical fetures:",(len(categorical_cols)))
# Select numerical columns
int_cols = [cname for cname in house_df.loc[:,:'SaleCondition'].columns if 
                house_df[cname].dtype in ['int64']]
    
print("Number of integer fetures:",(len(int_cols)))

float_cols = [cname for cname in house_df.loc[:,:'SaleCondition'].columns if 
                house_df[cname].dtype in ['float64']]
print("Number of float fetures:",(len(float_cols)))

print("int type features:")
plt.figure(figsize=(20, 20))
plt.subplots_adjust(hspace=0.5)
for i, var in enumerate(int_cols):
    plt.subplot(13,2,i+1)
    fig = train[var].hist(bins=60)
    fig.set_ylabel('number of houses')
    fig.set_xlabel(var)
    str3= "".join([var,' Histogram'])
    fig.set_title(str3)

print("float type features:")
plt.figure(figsize=(20, 20))
plt.subplots_adjust(hspace=0.5)
for i, var in enumerate(float_cols):
    plt.subplot(6,2,i+1)
    fig = train[var].hist(bins=60)
    fig.set_ylabel('number of houses')
    fig.set_xlabel(var)
    str3= "".join([var,' Histogram'])
    fig.set_title(str3)

print("Categorial type features:")
plt.figure(figsize=(30, 30))
plt.subplots_adjust(hspace=0.5)
for i, var in enumerate(categorical_cols):
    plt.subplot(11,4,i+1)
    fig = train[var].hist(bins=60)
    fig.set_ylabel('number of houses')
    fig.set_xlabel(var)
    str3= "".join([var,' Histogram'])
    fig.set_title(str3)

"""factorize categorial values

# Section 2: Feature Engineering

## 2.1 Dealing with Outliers

***outlier:***  a data point that is distant from other similar points.should be excluded from the data set. We'll do a quick analysis through the standard deviation of 'SalePrice' and a set of scatter plots.
"""

house_df[tr_idx]

# int colums scatter plot
plt.figure(figsize=(30, 30))
plt.subplots_adjust(hspace=0.5)
for i, var in enumerate(int_cols):
  plt.subplot(9,3,i+1)
  fig = sns.scatterplot(data=train, x=var, y='SalePrice')
  fig.set_ylabel("SalePrice", labelpad = 5)
  fig.set_xlabel(var)
  str3= "".join([var,'Scatter plot'])
  fig.set_title(str3)
plt.show()

# float colums scatter plot
plt.figure(figsize=(30, 30))
plt.subplots_adjust(hspace=0.5)
for i, var in enumerate(float_cols):
  plt.subplot(9,3,i+1)
  fig = sns.scatterplot(data=house_df[tr_idx],x=var,y='SalePrice')
  fig.set_ylabel("SalePrice", labelpad = 5)
  fig.set_xlabel(var)
  str3= "".join([var,'Scatter plot'])
  fig.set_title(str3)
plt.show()

# Amit: We dosent need this when useing the 'remove_Outlier' function
"""
index = (house_df[tr_idx]['LotArea'] > 100000)
index_list = list(house_df[tr_idx][index].index)
print(index_list)
house_df = house_df.drop(index_list)
house_df
"""

# Amit: outdated function, use the currect one
"""
def remove_Outlier(data,tr_idx ,feature_name, value):
  index = (data[tr_idx][feature_name] > value)
  index_list = list(data[tr_idx][index].index)
  data = data.drop(index_list)
  return data
  """

def remove_Outlier(data, feature_name, value):
  idx = (data[feature_name] > value)
  index_list = list(data[idx].index)
  data = data.drop(index_list)
  return data

"""**lets Analyze where the outliers are hidding:**

by observing the scatter praphs above we can notice the following ouliers data points:

**For integer represented features**:

1. LotArea feature - over 100000
2. 1stFlrSF feature - over 2500
3. 2ndFlrSF feature - over 1750
4. GrLiveArea feature - over 4000
5. WoodDeckSF feature - over 700
6. PoolArea feature - over 400
7. MiscVal feature - over 8000
8. EnclosedPorch feature - over 350

# int cols
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "LotArea", 100000)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "1stFlrSF", 2500)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "2ndFlrSF", 1750)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "GrLivArea", 4000)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "WoodDeckSF", 700)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "PoolArea", 400)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "MiscVal", 8000)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "EnclosedPorch", 350)
# float cols
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "MasVnrArea", 1200)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "BsmtFinSF1", 2000)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "BsmtFinSF2", 1200)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "TotalBsmtSF", 3000)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "GarageArea", 1100)

# int cols
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "LotArea", 100000)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "1stFlrSF", 2500)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "2ndFlrSF", 1750)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "GrLivArea", 4000)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "WoodDeckSF", 700)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "PoolArea", 400)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "MiscVal", 8000)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "EnclosedPorch", 350)
# float cols
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "MasVnrArea", 1200)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "BsmtFinSF1", 2000)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "BsmtFinSF2", 1200)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "TotalBsmtSF", 3000)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "GarageArea", 1100)
"""

plt.figure(figsize=(30, 10))
plt.subplots_adjust(hspace=1)
for i, var in enumerate(["LotArea", "1stFlrSF", "2ndFlrSF", "GrLivArea", "WoodDeckSF", "PoolArea", "MiscVal", "EnclosedPorch"]):
  plt.subplot(2,8,i+1)
  fig = sns.scatterplot(data=house_df[tr_idx],x=var,y='SalePrice')
  fig.set_ylabel("SalePrice", labelpad = 5)
  fig.set_xlabel(var)
  fig.set_title("before removing Outliers")
plt.show()
# int cols
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "LotArea", 10000)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "1stFlrSF", 2500)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "2ndFlrSF", 1750)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "GrLivArea", 4000)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "WoodDeckSF", 700)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "PoolArea", 400)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "MiscVal", 8000)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "EnclosedPorch", 350)

plt.figure(figsize=(30, 10))
plt.subplots_adjust(hspace=1)
for i, var in enumerate(["LotArea", "1stFlrSF", "2ndFlrSF", "GrLivArea", "WoodDeckSF", "PoolArea", "MiscVal", "EnclosedPorch"]):
  plt.subplot(2,8,i+9)
  fig = sns.scatterplot(data=house_df[tr_idx],x=var,y='SalePrice')
  fig.set_ylabel("SalePrice", labelpad = 5)
  fig.set_xlabel(var)
  fig.set_title("After removing Outliers")
plt.show()

float_cols

"""**For float represented features**:

1. MasVnrArea feature - over 1200
2. BsmtFinSF1 feature - over 2000
3. BsmtFinSF2 feature - over 1200
4. TotalBsmtSF feature - over 3000
5. GarageArea feature - over 1100
"""

plt.figure(figsize=(30, 10))
plt.subplots_adjust(hspace=1)
float_Outliers = ["MasVnrArea", "BsmtFinSF1", "BsmtFinSF2", "TotalBsmtSF", "GarageArea"]
for i, var in enumerate(float_Outliers):
  plt.subplot(2,len(float_Outliers),i+1)
  fig = sns.scatterplot(data=house_df[tr_idx],x=var,y='SalePrice')
  fig.set_ylabel("SalePrice", labelpad = 5)
  fig.set_xlabel(var)
  fig.set_title("before removing Outliers")
plt.show()

# float cols
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "MasVnrArea", 1200)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "BsmtFinSF1", 2000)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "BsmtFinSF2", 1200)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "TotalBsmtSF", 3000)
house_df[tr_idx] = remove_Outlier(house_df[tr_idx], "GarageArea", 1100)

plt.figure(figsize=(30, 10))
plt.subplots_adjust(hspace=1)
for i, var in enumerate(float_Outliers):
  plt.subplot(2, len(float_Outliers),i + len(float_Outliers) + 1)
  fig = sns.scatterplot(data=house_df[tr_idx],x=var,y='SalePrice')
  fig.set_ylabel("SalePrice", labelpad = 5)
  fig.set_xlabel(var)
  fig.set_title("After removing Outliers")
plt.show()

"""When removing outliers according to the int colum, we notice that other outlier has been remove (according to other colums). The reasone behide it is that some samples conation multiple outliers according to different features. This is another reason to throw outlier samples.  

After applying the outliers removal severl times for different features, we notice that the most beneficial removal of outliers contain the following features as described in function: 

1. LotFrontage > 200
2. BsmtFinSF1 > 2000
3. 1stFlrSF > 2500
4. TotalBsmtSF > 3000
5. GrLivArea > 4000
6. LotArea > 100000

"""

def Remove_all_ourliers(data):
  data = remove_Outlier(data, "LotFrontage", 200)
  data = remove_Outlier(data, "BsmtFinSF1", 2000)
  data = remove_Outlier(data, "1stFlrSF", 2500)
  data = remove_Outlier(data, "TotalBsmtSF", 3000)
  data = remove_Outlier(data, "GrLivArea", 4000)
  data = remove_Outlier(data, "LotArea", 100000)
  return data

# Creating new train, test and house_df after droping outliers samples
train_rm_o = Remove_all_ourliers(train)
test_rm_o = Remove_all_ourliers(test)
house_df_rm_o = pd.concat([train_rm_o,test_rm_o],ignore_index = True, sort = False)
tr_idx_rm = house_df_rm_o['SalePrice'].notnull() ## display data with on
te_idx_rm = [not elem for elem in tr_idx]
te_idx_rm = pd.Series(te_idx)

"""## 2.2 Missing Values

https://scikit-learn.org/stable/auto_examples/impute/plot_missing_values.html#sphx-glr-auto-examples-impute-plot-missing-values-py
"""

# train_rm_o
# test_rm_o
# house_df_rm_o
# tr_idx_rm
# te_idx_rm


X_train = house_df_rm_o[tr_idx_rm].drop(['SalePrice'], axis=1)
X_test = house_df_rm_o[te_idx_rm]
Y_train = house_df_rm_o[tr_idx_rm].SalePrice
Y_test = house_df_rm_o[te_idx_rm].SalePrice
overall_X = house_df_rm_o
print(X_train.shape)
print(Y_train.shape)
print(X_test.shape)
print(Y_test.shape)
print(overall_X.shape)

len(categorical_cols)

"""### 2.2.1 Drop columns with al lot of missing values

Removing all the sample with missing values ssually isn't the best solution.  However, it can be useful when most values in a column are missing.
"""

missing_data

missing_features = [col for col in X_train.columns if (X_train[col].isnull().sum() / X_train.shape[0]) * 100 > 50]
missing_features
print('missing_features: {}'.format(missing_features))
X_df_drop = overall_X.drop(missing_features, axis=1)
X_train_drop = X_df_drop[tr_idx]
X_test_drop = X_df_drop[te_idx]

print('X_df_drop shape:     {}'.format(X_df_drop.shape))
print('X_train_drop shape:  {}'.format(X_train_drop.shape))
print('X_test_drop shape:   {}'.format(X_test_drop.shape))
print(type(tr_idx))

missing_features

categorical_cols_1 = categorical_cols
print(categorical_cols_1)
for item in missing_features:
  print(item)
  categorical_cols_1.remove(item)
categorical_cols_1

len(categorical_cols)

"""### 2.2.2 Categorical colums: replacing with statistical value"""

X = X_df_drop
pd.set_option('display.max_rows', None)
missing_data = pd.DataFrame(X.isna().sum())
missing_data_prec = missing_data/X.shape[0] * 100
missing_data = pd.concat([missing_data,missing_data_prec], axis=1)
missing_data.columns =['occurence', 'precentage']
print(missing_data)
pd.reset_option('all')

"""we should try to apply the missing values algorithms for the concatanate data, and on the train data by itself."""

from sklearn.impute import SimpleImputer, KNNImputer
class Impute_class(object):
  def __init__(self, X):
    self.X = X

  def impute_values(self, cols, method, K = 2):
    data = self.X
    self.method = method
    self.K = K
    imputer = self.impute_method()
    imputer.fit(self.X)
    new_X = pd.DataFrame(imputer.fit_transform(self.X[cols]))
    new_X.cols = self.X[cols].columns
    data[cols] = new_X
    return data

  def impute_method(self):
    if self.method == "zero":
      imputer = SimpleImputer(add_indicator=False, strategy="constant", fill_value=0)

    elif self.method == "mean":
      imputer = SimpleImputer(add_indicator=False, strategy="mean")

    elif self.method == "median":
      imputer = SimpleImputer(add_indicator=False, strategy="median")
      
    elif self.method == "most_frequent":
      imputer = SimpleImputer(add_indicator=False, strategy="most_frequent")

    elif self.method == "KNN":
      imputer = KNNImputer(n_neighbors=self.K, weights="uniform")
      
    return imputer

X = X_df_drop
pd.set_option('display.max_rows', None)
missing_data = pd.DataFrame(X.isna().sum())
missing_data_prec = missing_data/X.shape[0] * 100
missing_data = pd.concat([missing_data,missing_data_prec], axis=1)
missing_data.columns =['occurence', 'precentage']
print(missing_data)
pd.reset_option('all')

I = Impute_class(X=X)
I.impute_values(cols=categorical_cols, method= "most_frequent")

pd.set_option('display.max_rows', None)
missing_data = pd.DataFrame(X.isna().sum())
missing_data_prec = missing_data/X.shape[0] * 100
missing_data = pd.concat([missing_data,missing_data_prec], axis=1)
missing_data.columns =['occurence', 'precentage']
print(missing_data)
pd.reset_option('all')

"""I = Impute_class(X=X)
# I.impute_values(cols=categorical_cols, method= "most_frequent")
I.impute_values(cols= int_cols, method= "median")
I.impute_values(cols= float_cols, method= "KNN", K=10)
X = I.X

pd.set_option('display.max_rows', None)
missing_data = pd.DataFrame(X.isna().sum())
missing_data_prec = missing_data/X.shape[0] * 100
missing_data = pd.concat([missing_data,missing_data_prec], axis=1)
missing_data.columns =['occurence', 'precentage']
print(missing_data)
pd.reset_option('all')

## 2.3 Encoding categorical features
"""

# tr_idx_rm 
# te_idx_rm 

house_df_X = X
train_X = house_df_X[tr_idx_rm]
test_X = house_df_X[te_idx_rm]

print("Categorial type features:")
plt.figure(figsize=(30, 30))
plt.subplots_adjust(hspace=0.5)
for i, var in enumerate(categorical_cols):
    plt.subplot(11,4,i+1)
    fig = house_df_X[tr_idx_rm][var].hist(bins=60)
    fig.set_ylabel('number of houses')
    fig.set_xlabel(var)
    str3= "".join([var,' Histogram'])
    fig.set_title(str3)

"""### 2.3.1 Factorizing categorical features"""

def factorization(data, categorical_cols):
  for c in categorical_cols:
    data[c] = pd.factorize(data[c])[0]
  return data

house_df_fac = house_df_X
house_df_fac = factorization(house_df_fac, categorical_cols)

print("Categorial type features:")
plt.figure(figsize=(30, 30))
plt.subplots_adjust(hspace=0.5)
for i, var in enumerate(categorical_cols):
    plt.subplot(11,4,i+1)
    fig = house_df_fac[tr_idx_rm][var].hist(bins=60)
    fig.set_ylabel('number of houses')
    fig.set_xlabel(var)
    str3= "".join([var,' Histogram'])
    fig.set_title(str3)

"""Due to supiriorty of the lable incoding, we will not use this method.

### 2.3.2 Label encoding categorical features

Another option is using Label Encoding which refers to transforming the word labels into numerical form so that the algorithms can understand how to operate on them.
"""

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
def Labaling(data, categorical_cols):
  for c in categorical_cols: 
    lbl = LabelEncoder()
    lbl.fit(list(data[c].values)) 
    data[c] = lbl.transform(list(data[c].values))
  return data

house_X_lab = Labaling(house_df_X, categorical_cols)
print(categorical_cols)
print("Categorial type features:")
plt.figure(figsize=(30, 30))
plt.subplots_adjust(hspace=0.5)
for i, var in enumerate(categorical_cols):
    plt.subplot(11,4,i+1)
    fig = house_X_lab[tr_idx_rm][var].hist(bins=60)
    fig.set_ylabel('number of houses')
    fig.set_xlabel(var)
    str3= "".join([var,' Histogram'])
    fig.set_title(str3)

"""### 2.3.3 One-hot encoding categorical features
We apply One-Hot Encoding when:

The categorical feature is not ordinal (like the countries above)
The number of categorical features is less so one-hot encoding can be effectively applied
We apply Label Encoding when:

The categorical feature is ordinal (like Jr. kg, Sr. kg, Primary school, high school)
 The number of categories is quite large as one-hot encoding can lead to high memory consumption

## 2.4 Missing Values - int and float colums
"""

I = Impute_class(X=X)
# I.impute_values(cols=categorical_cols, method= "most_frequent")
I.impute_values(cols= int_cols, method= "median")
I.impute_values(cols= float_cols, method= "KNN", K=10)
X = I.X

pd.set_option('display.max_rows', None)
missing_data = pd.DataFrame(X.isna().sum())
missing_data_prec = missing_data/X.shape[0] * 100
missing_data = pd.concat([missing_data,missing_data_prec], axis=1)
missing_data.columns =['occurence', 'precentage']
print(missing_data)
pd.reset_option('all')

"""## 2.5 Taking care of skewed target

The Sale price is a bit skewed, and for linear models its important that you normalize your data, so im going to log transform it
"""

# X_train = house_df_rm_o[tr_idx_rm].drop(['SalePrice'], axis=1)
# X_test = house_df_rm_o[te_idx_rm]
# Y_train = house_df_rm_o[tr_idx_rm].SalePrice
# Y_test = house_df_rm_o[te_idx_rm].SalePrice

house_df_X = X
house_df_X
train_X = house_df_X[tr_idx_rm]
test_X = house_df_X[te_idx_rm]

Y_price = house_df_rm_o[tr_idx_rm].SalePrice
Y_price

plt.figure()
sns.distplot(Y_price);
Y_price_log = np.log1p(Y_price)
plt.title('Target SalePrice - skewed')
plt.figure()
sns.distplot(Y_price_log)
plt.title('Log Transform Target SalePrice - unskewed')

"""plt.figure()
sns.distplot(house_df_X['SalePrice']);
house_df_X['SalePrice'] = np.log1p(house_df_X['SalePrice'])
plt.title('Target SalePrice - skewed')
plt.figure()
sns.distplot(house_df_X['SalePrice'])
plt.title('Log Transform Target SalePrice - unskewed')

train_X = house_df_X[tr_idx_rm]
test_X = house_df_X[te_idx_rm]

##2.6 Normalize features

https://www.width.ai/pandas/normalize-column-pandas-dataframe

There is a problem here since all elements became float type - its sh
"""

from scipy import stats
# X_train_imp_norm =  pd.DataFrame(stats.zscore(X))
# X_train_imp_norm.columns = X.columns
# X_train_imp_norm

# house_df_X = X
# house_df_X
# train_X = house_df_X[tr_idx_rm]
# test_X = house_df_X[te_idx_rm]

# house_df_X_norm = pd.DataFrame(stats.zscore(house_df_X))
# house_df_X_norm.columns = house_df_X.columns

"""from sklearn.cluster import KMeans
kmeans = KMeans(3,init='k-means++')
kmeans.fit(df_preprocessed.drop('species',axis=1))
wcss=[]
for i in range(1,10):
    kmeans = KMeans(i)
    kmeans.fit(df_preprocessed.drop('species',axis=1))
    pred_i = kmeans.labels_
    wcss.append(kmeans.inertia_)

plt.figure(figsize=(10,6))
plt.plot(range(1,10),wcss)
plt.ylim([0,1800])
plt.title('The Elbow Method',{'fontsize':20})
plt.xlabel('Number of clusters')
plt.ylabel('Within-cluster Sum of Squares');

new_X_train

def optimize_k(data, target):
    errors = []
    for k in range(1, 20, 2):
        imputer = KNNImputer(n_neighbors=k)
        imputed = imputer.fit_transform(data)
        df_imputed = pd.DataFrame(imputed, columns=df.columns)
        
        X = df_imputed.drop(target, axis=1)
        y = df_imputed[target]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        model = RandomForestRegressor()
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        error = rmse(y_test, preds)
        errors.append({'K': k, 'RMSE': error})
        
    return errors
k_errors = optimize_k(data=df, target='MEDV')

## 2.7 Corrulations

Relationship between SalePrice and other numerical columns
"""

#correlation matrix
corrmat = train.corr()
f, ax = plt.subplots(figsize=(16, 14))
sns.heatmap(corrmat, vmax=.8, square=True);

"""# Section 3: Implement prediction models

## 3.1 Model implemention
"""

from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LassoCV
from sklearn.kernel_ridge import KernelRidge
from sklearn.linear_model import Ridge
from sklearn.linear_model import RidgeCV
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score

class Regression_model(object):
    def __init__(self, X, Y):
        self.X = X
        self.Y = Y

    def Linear_reg(self):
        self.model = LinearRegression(normalize=True)
        self.model.fit(self.X, self.Y)

    def Lasso_reg(self, alphas):
        self.model = LassoCV(alphas=alphas)
        self.model.fit(self.X, self.Y)
        
    def Ridge_reg(self, alpha):
        self.model = Ridge(alpha)
        self.model.fit(self.X, self.Y)

    def KernelRidge_reg(self, alpha):
        self.model = KernelRidge(alpha)
        self.model.fit(self.X, self.Y)

    def RidgeKfold_reg(self, alphas):
        # should enter a list of alphas - algorithem uses the most beneficial
        # alphas = [1e-4, 1e-3, 1e-2, 1e-1, 1] 
        self.model = RidgeCV(alphas)
        self.model.fit(self.X, self.Y)

    def Poly_reg(self):
        self.model = PolynomialFeatures(interaction_only=True)
        self.model.fit(self.X, self.Y)

    def score_dataset(self, X_test, y_test):
        preds = self.model.predict(X_test)
        MAE = mean_absolute_error(y_test, preds)
        R2 = r2_score(y_test,y_pred=preds)
        RMSE = np.sqrt(mean_squared_error(y_test,y_pred=preds))
        # Score = self.model.score(X_test, y_test)
        print("MAE = {}".format(MAE))
        print("R2 = {}".format(R2))
        print("RMSE = {}".format(RMSE))
        # print("Score = {}".format(Score))
        return MAE, R2, RMSE

"""## 3.2 Data Split

Splitting the original train data into train and validation 

consider use k-fold
"""

from sklearn.model_selection import train_test_split

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score

X = house_df_X.drop(['SalePrice'], axis=1)
Y_train = Y_price_log
X_train = X[tr_idx_rm]
X_test = [te_idx_rm]


print(X.shape)
print(Y_train.shape)
Xtrain, Xvalid, ytrain, yvalid = train_test_split(X_train, Y_train, test_size = 0.3, random_state = 0)
print("Xtrain : " + str(Xtrain.shape))
print("Xtest : " + str(Xvalid.shape))
print("ytrain : " + str(ytrain.shape))
print("ytest : " + str(yvalid.shape))

"""https://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/

## 3.2 Evaluate models
"""

MAE_array_train = []
MAE_array_valid = []
R2_array_train = []
R2_array_valid = []
RMSE_array_train = []
RMSE_array_valid = []

R = Regression_model(Xtrain, ytrain)
print("Linear Regression Model results:")
R.Linear_reg()
print("performences over Train set:")
Score = R.score_dataset(Xtrain, ytrain)
MAE_array_train.append(Score[0])
R2_array_train.append(Score[1])
RMSE_array_train.append(Score[2])
print("\n")

print("performences over Validation set:")
Score = R.score_dataset(Xvalid, yvalid)
MAE_array_valid.append(Score[0])
R2_array_valid.append(Score[1])
RMSE_array_valid.append(Score[2])
print("\n")

print("Lasso Regression Model results:")
R.Lasso_reg([1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])
print("performences over Train set:")
Score = R.score_dataset(Xtrain, ytrain)
MAE_array_train.append(Score[0])
R2_array_train.append(Score[1])
RMSE_array_train.append(Score[2])
print("\n")

print("performences over Validation set:")
Score = R.score_dataset(Xvalid, yvalid)
MAE_array_valid.append(Score[0])
R2_array_valid.append(Score[1])
RMSE_array_valid.append(Score[2])
print("\n")

# print("Kernal Ridge Regression Model results:")
# K_reg = R.KernelRidge_reg(10)
# print("performences over Train set:")
# R.score_dataset(Xtrain, ytrain)
# print("\n")
# print("performences over Validation set:")
# R.score_dataset(Xvalid, yvalid)
# print("\n")

R.RidgeKfold_reg([1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])
print("Ridge Kfold Regression Model results:")
print("performences over Train set:")
Score = R.score_dataset(Xtrain, ytrain)
MAE_array_train.append(Score[0])
R2_array_train.append(Score[1])
RMSE_array_train.append(Score[2])
print("\n")

print("performences over Validation set:")
Score = R.score_dataset(Xvalid, yvalid)
MAE_array_valid.append(Score[0])
R2_array_valid.append(Score[1])
RMSE_array_valid.append(Score[2])
print("\n")

fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=False)
axs[0].scatter(["Linear", "Lasso", "Ridge"], MAE_array_valid)
axs[1].scatter(["Linear", "Lasso", "Ridge"], R2_array_valid)
axs[2].scatter(["Linear", "Lasso", "Ridge"], RMSE_array_valid)
axs[0].set_ylabel("MAE")
axs[1].set_ylabel("R2")
axs[2].set_ylabel("RMSE")
axs[0].set_xlabel("Regression method")
axs[1].set_xlabel("Regression method")
axs[2].set_xlabel("Regression method")
fig.suptitle('Regression methods Performences')
plt.show()

"""Playing with the KNN imputation - will take onlr Ridge and Lasso regression

### 3.2.1 initialization function
"""

# Amit initialize function
import numpy as np
import pandas as pd

# param for later: impCat, impInt, impFloat, kCat = 10, kInt = 10, kFloat = 10
def initialize_data():
    train = pd.read_csv(main_path + '/train.csv')
    test = pd.read_csv(main_path + '/test.csv')
    
    ## missing values - remove samples
    train = Remove_all_ourliers(train)
    test = Remove_all_ourliers(test)

    # indexing
    house_df = pd.concat([train,test],ignore_index = True, sort = False)
    tr_idx = house_df['SalePrice'].notnull()
    te_idx = [not elem for elem in tr_idx]
    te_idx = pd.Series(te_idx)

    X_train = house_df[tr_idx].drop(['SalePrice'], axis=1)
    # X_test = house_df[te_idx].drop(['SalePrice'], axis=1)


    ## missing features - remove features 
    missing_features = [col for col in X_train.columns if (X_train[col].isnull().sum() / X_train.shape[0]) * 100 > 50]
    X_df_drop = house_df.drop(missing_features, axis=1)

    X_train_drop = X_df_drop[tr_idx]
    X_test_drop = X_df_drop[te_idx]
    Y_price = X_df_drop.SalePrice
    
    ## categorical missing features - impute features
    categorical_cols = [cname for cname in X_df_drop.loc[:,:'SaleCondition'].columns if
                    X_df_drop[cname].nunique() < 200 and 
                    X_df_drop[cname].dtype == "object"]
    
    I = Impute_class(X=X_df_drop)
    I.impute_values(cols=categorical_cols, method= "most_frequent")
    X_df_impute = I.X

    ## Categorical lable encoding
    X_df_lable = Labaling(X_df_impute, categorical_cols)

    ## int and float missing features - impute
    I = Impute_class(X=X_df_lable)
    I.impute_values(cols= int_cols, method= "median")
    I.impute_values(cols= float_cols, method= "KNN", K=10)
    X_df_impute2 = I.X

    ## Taking care of skewed target
    Y_price_log = np.log1p(Y_price)

    # indexing
    X_train = X_df_impute2[tr_idx].drop(['SalePrice'], axis=1)
    Y_train = Y_price_log[tr_idx]
    te_idx = [not elem for elem in tr_idx]
    te_idx = pd.Series(te_idx)
    X_test = np.log1p(Y_price)


    # return X_train, Y_train, X_test, Y_test, categorical_cols, tr_idx, te_idx
    return X_train, Y_train, X_test

x_train, y_train, x_test = initialize_data()

print(x_train.shape)
print(y_train.shape)
Xtrain, Xvalid, ytrain, yvalid = train_test_split(x_train, y_train, test_size = 0.3, random_state = 0)
print("Xtrain : " + str(Xtrain.shape))
print("Xtest : " + str(Xvalid.shape))
print("ytrain : " + str(ytrain.shape))
print("ytest : " + str(yvalid.shape))

MAE_array_train = []
MAE_array_valid = []
R2_array_train = []
R2_array_valid = []
RMSE_array_train = []
RMSE_array_valid = []

R = Regression_model(Xtrain, ytrain)
print("Linear Regression Model results:")
R.Linear_reg()
print("performences over Train set:")
Score = R.score_dataset(Xtrain, ytrain)
MAE_array_train.append(Score[0])
R2_array_train.append(Score[1])
RMSE_array_train.append(Score[2])
print("\n")

print("performences over Validation set:")
Score = R.score_dataset(Xvalid, yvalid)
MAE_array_valid.append(Score[0])
R2_array_valid.append(Score[1])
RMSE_array_valid.append(Score[2])
print("\n")

print("Lasso Regression Model results:")
R.Lasso_reg([1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])
print("performences over Train set:")
Score = R.score_dataset(Xtrain, ytrain)
MAE_array_train.append(Score[0])
R2_array_train.append(Score[1])
RMSE_array_train.append(Score[2])
print("\n")

print("performences over Validation set:")
Score = R.score_dataset(Xvalid, yvalid)
MAE_array_valid.append(Score[0])
R2_array_valid.append(Score[1])
RMSE_array_valid.append(Score[2])
print("\n")

# print("Kernal Ridge Regression Model results:")
# K_reg = R.KernelRidge_reg(10)
# print("performences over Train set:")
# R.score_dataset(Xtrain, ytrain)
# print("\n")
# print("performences over Validation set:")
# R.score_dataset(Xvalid, yvalid)
# print("\n")

R.RidgeKfold_reg([1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])
print("Ridge Kfold Regression Model results:")
print("performences over Train set:")
Score = R.score_dataset(Xtrain, ytrain)
MAE_array_train.append(Score[0])
R2_array_train.append(Score[1])
RMSE_array_train.append(Score[2])
print("\n")

print("performences over Validation set:")
Score = R.score_dataset(Xvalid, yvalid)
MAE_array_valid.append(Score[0])
R2_array_valid.append(Score[1])
RMSE_array_valid.append(Score[2])
print("\n")

fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=False)
axs[0].scatter(["Linear", "Lasso", "Ridge"], MAE_array_valid)
axs[1].scatter(["Linear", "Lasso", "Ridge"], R2_array_valid)
axs[2].scatter(["Linear", "Lasso", "Ridge"], RMSE_array_valid)
axs[0].set_ylabel("MAE")
axs[1].set_ylabel("R2")
axs[2].set_ylabel("RMSE")
axs[0].set_xlabel("Regression method")
axs[1].set_xlabel("Regression method")
axs[2].set_xlabel("Regression method")
fig.suptitle('Regression methods Performences')
plt.show()

# Dor initialize function
"""
import numpy as np
import pandas as pd
def initialize_data():
    train = pd.read_csv(main_path + '/train.csv')
    test = pd.read_csv(main_path + '/test.csv')
    house_df = pd.concat([train,test],ignore_index = True, sort = False)
    tr_idx = house_df['SalePrice'].notnull()
    te_idx = [not elem for elem in tr_idx]
    te_idx = pd.Series(te_idx)
    # pd.reset_option('all')
    
    # Dropping columns with higher missimg data rate
    missing_data = [col for col in train.columns if (train[col].isnull().sum() / train.shape[0]) * 100 > 50]
    house_df = house_df.drop(missing_data, axis=1)

    # Taking care of skewed target
    house_df['SalePrice'] = np.log1p(house_df['SalePrice'])
    train = house_df[tr_idx]
    test = house_df[tr_idx]
  
    # Taking care of Outliers
    train = Remove_all_ourliers(train)

    house_df = pd.concat([train,test],ignore_index = True, sort = False)
    tr_idx = house_df['SalePrice'].notnull()

    # factorize
    categorical_cols = [cname for cname in house_df.loc[:,:'SaleCondition'].columns if
                    house_df[cname].nunique() < 200 and 
                    house_df[cname].dtype == "object"]

    house_df = Labaling(house_df, categorical_cols)
    X_train = house_df[tr_idx].drop(['SalePrice'], axis=1)
    Y_train = house_df[tr_idx].SalePrice
    te_idx = [not elem for elem in tr_idx]
    te_idx = pd.Series(te_idx)
    X_test = house_df[te_idx]
    Y_test = house_df[te_idx].SalePrice
    
    return X_train, Y_train, X_test, Y_test, categorical_cols, tr_idx, te_idx
"""

"""### ++++++++++"""

fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=False)
for K_it in np.linspace(2,50,25, dtype= int):
  # X, Y_train, X_test, Y_test = initialize_data("most_frequent", "KNN", "KNN",K_it, K_it, K_it)
  X_1, Y_train, X_test, Y_test = initialize_data(impCat = "most_frequent", impInt = "KNN", impFloat = "KNN", Kcat = K_it, Kint = K_it, Kfloat = K_it)
  # I = Impute_class(X)
  # I.impute_values(cols=categorical_cols, method= "KNN",K=K_it)
  # I.impute_values(cols=int_cols, method= "KNN",K=K_it)
  # I.impute_values(cols=float_cols, method= "KNN",K=K_it)
  # X = I.X
  # # normalize features
  # X_norm =  pd.DataFrame(stats.zscore(X_1))
  # X_norm.columns = X_1.columns

  Xtrain, Xvalid, ytrain, yvalid = train_test_split(X_1, Y_train, test_size = 0.3, random_state = 0)
  R = Regression_model(Xtrain, ytrain)
  MAE_array_train = []
  MAE_array_valid = []
  R2_array_train = []
  R2_array_valid = []
  RMSE_array_train = []
  RMSE_array_valid = []
  print("K = {}".format(K_it))
  R.Lasso_reg([1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])
  Score = R.score_dataset(Xvalid, yvalid)
  MAE_array_valid.append(Score[0])
  R2_array_valid.append(Score[1])
  RMSE_array_valid.append(Score[2])

  R.RidgeKfold_reg([1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])
  Score = R.score_dataset(Xvalid, yvalid)
  MAE_array_valid.append(Score[0])
  R2_array_valid.append(Score[1])
  RMSE_array_valid.append(Score[2])

  axs[0].scatter(["Lasso", "Ridge"], MAE_array_valid, label ="K ={}".format(K_it))
  axs[1].scatter(["Lasso", "Ridge"], R2_array_valid, label ="K ={}".format(K_it))
  axs[2].scatter(["Lasso", "Ridge"], RMSE_array_valid, label ="K ={}".format(K_it))
axs[0].set_ylabel("MAE")
axs[1].set_ylabel("R2")
axs[2].set_ylabel("RMSE")
axs[0].set_xlabel("Regression method")
axs[1].set_xlabel("Regression method")
axs[2].set_xlabel("Regression method")
fig.suptitle('Regression methods Performences')
plt.legend()
plt.show()

fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=False)
for K_it in np.linspace(2,50,25, dtype= int):
  X, Y_train, X_test, Y_test = initialize_data(train, test, categorical_cols)
  I = Impute_class(X)
  I.impute_values(cols=categorical_cols, method= "KNN",K=K_it)
  I.impute_values(cols=int_cols, method= "KNN",K=K_it)
  I.impute_values(cols=float_cols, method= "KNN",K=K_it)
  X = I.X
  # normalize features
  X_train_imp_norm =  pd.DataFrame(stats.zscore(X))
  X_train_imp_norm.columns = X.columns
  
  Xtrain, Xvalid, ytrain, yvalid = train_test_split(X_train_imp_norm[tr_idx], Y_train, test_size = 0.3, random_state = 0)
  R = Regression_model(Xtrain, ytrain)
  MAE_array_train = []
  MAE_array_valid = []
  R2_array_train = []
  R2_array_valid = []
  RMSE_array_train = []
  RMSE_array_valid = []
  print("K = {}".format(K_it))
  R.Lasso_reg([1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])
  Score = R.score_dataset(Xvalid, yvalid)
  MAE_array_valid.append(Score[0])
  R2_array_valid.append(Score[1])
  RMSE_array_valid.append(Score[2])

  R.RidgeKfold_reg([1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])
  Score = R.score_dataset(Xvalid, yvalid)
  MAE_array_valid.append(Score[0])
  R2_array_valid.append(Score[1])
  RMSE_array_valid.append(Score[2])

  axs[0].scatter(["Lasso", "Ridge"], MAE_array_valid, label ="K ={}".format(K_it))
  axs[1].scatter(["Lasso", "Ridge"], R2_array_valid, label ="K ={}".format(K_it))
  axs[2].scatter(["Lasso", "Ridge"], RMSE_array_valid, label ="K ={}".format(K_it))
axs[0].set_ylabel("MAE")
axs[1].set_ylabel("R2")
axs[2].set_ylabel("RMSE")
axs[0].set_xlabel("Regression method")
axs[1].set_xlabel("Regression method")
axs[2].set_xlabel("Regression method")
fig.suptitle('Regression methods Performences')
plt.legend()
plt.show()

"""We can notice that Lasso regression performes way better than Ridge Regression"""

fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=False)
for K_it in np.linspace(2,50,50, dtype= int):
  X = initialize_data(train, test, categorical_cols)
  I = Impute_class(X)
  I.impute_values(cols=categorical_cols, method= "KNN",K= K_it)
  I.impute_values(cols=int_cols, method= "KNN",K= K_it)
  I.impute_values(cols=float_cols, method= "KNN",K= K_it)
  X = I.X
  X_train_imp_norm =  pd.DataFrame(stats.zscore(X))
  X_train_imp_norm.columns = X.columns
  Xtrain, Xvalid, ytrain, yvalid = train_test_split(X_train_imp_norm[tr_idx], Y_train, test_size = 0.3, random_state = 0)
  R = Regression_model(Xtrain, ytrain)
  print("K = {}".format(K_it))
  R.Lasso_reg([1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])
  Score = R.score_dataset(Xvalid, yvalid)
  MAE_array_valid.append(Score[0])
  R2_array_valid.append(Score[1])
  RMSE_array_valid.append(Score[2])

  axs[0].plot(K_it, Score[0],"o", label ="K ={}".format(K_it))
  axs[1].plot(K_it, Score[1],"o", label ="K ={}".format(K_it))
  axs[2].plot(K_it, Score[2],"o", label ="K ={}".format(K_it))
axs[0].set_ylabel("MAE")
axs[1].set_ylabel("R2")
axs[2].set_ylabel("RMSE")
axs[0].set_xlabel("Lasso Regression")
axs[1].set_xlabel("Lasso Regression")
axs[2].set_xlabel("Lasso Regression")
fig.suptitle('Regression methods Performences - Lasso')
plt.legend()
plt.show()

fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=False)
X = initialize_data(train, test, categorical_cols)
I = Impute_class(X)
I.impute_values(cols=categorical_cols, method= "most_frequent")
I.impute_values(cols=int_cols, method= "mean")
I.impute_values(cols=float_cols, method= "median",)
X = I.X
X_train_imp_norm =  pd.DataFrame(stats.zscore(X))
X_train_imp_norm.columns = X.columns
Xtrain, Xvalid, ytrain, yvalid = train_test_split(X_train_imp_norm[tr_idx], Y_train, test_size = 0.3, random_state = 0)
R = Regression_model(Xtrain, ytrain)
R.Lasso_reg([1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])
Score = R.score_dataset(Xvalid, yvalid)
MAE_array_valid.append(Score[0])
R2_array_valid.append(Score[1])
RMSE_array_valid.append(Score[2])
axs[0].plot(K_it, Score[0],"o", label ="K ={}".format(K_it))
axs[1].plot(K_it, Score[1],"o", label ="K ={}".format(K_it))
axs[2].plot(K_it, Score[2],"o", label ="K ={}".format(K_it))
axs[0].set_ylabel("MAE")
axs[1].set_ylabel("R2")
axs[2].set_ylabel("RMSE")
axs[0].set_xlabel("Regression method")
axs[1].set_xlabel("Regression method")
axs[2].set_xlabel("Regression method")
fig.suptitle('Regression methods Performences - Ridge')
# plt.legend()
plt.show()

"""K = 10
MAE = 0.09197296137523692
R2 = 0.7997209415512774
RMSE = 0.17591184162060375

K = 8
MAE = 0.09194339972473553
R2 = 0.7992661154371496
RMSE = 0.17611147289249104

No KNN:
MAE = 0.09198478447624521
R2 = 0.7993810645978225
RMSE = 0.17606104103589937

best resukts given by using K = 10

*Only Categorical Variabels using KNN:*
"""

fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=False)
for K_it in np.linspace(2,50,25, dtype= int):
  X = initialize_data(train, test, categorical_cols)
  I = Impute_class(X)
  I.impute_values(cols=categorical_cols, method= "KNN",K= K_it)
  I.impute_values(cols=int_cols, method= "mean")
  I.impute_values(cols=float_cols, method= "median")
  X = I.X
  X_train_imp_norm =  pd.DataFrame(stats.zscore(X))
  X_train_imp_norm.columns = X.columns
  Xtrain, Xvalid, ytrain, yvalid = train_test_split(X_train_imp_norm[tr_idx], Y_train, test_size = 0.3, random_state = 0)
  print("K = {}".format(K_it))
  R = Regression_model(Xtrain, ytrain)
  R.Lasso_reg([1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])
  Score = R.score_dataset(Xvalid, yvalid)
  MAE_array_valid.append(Score[0])
  R2_array_valid.append(Score[1])
  RMSE_array_valid.append(Score[2])
  axs[0].plot(K_it, Score[0],"o", label ="K ={}".format(K_it))
  axs[1].plot(K_it, Score[1],"o", label ="K ={}".format(K_it))
  axs[2].plot(K_it, Score[2],"o", label ="K ={}".format(K_it))
axs[0].set_ylabel("MAE")
axs[1].set_ylabel("R2")
axs[2].set_ylabel("RMSE")
axs[0].set_xlabel("Regression method")
axs[1].set_xlabel("Regression method")
axs[2].set_xlabel("Regression method")
fig.suptitle('Regression methods Performences - Lasso')
plt.legend()
plt.show()

"""*Only int Variabels using KNN:*"""

fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=False)
for K_it in np.linspace(2,50,25, dtype= int):
  X = initialize_data(train, test, categorical_cols)
  I = Impute_class(X)
  I.impute_values(cols=categorical_cols, method= "most_frequent")
  I.impute_values(cols=int_cols, method= "KNN",K= K_it)
  I.impute_values(cols=float_cols, method= "median")
  X = I.X
  X_train_imp_norm =  pd.DataFrame(stats.zscore(X))
  X_train_imp_norm.columns = X.columns
  Xtrain, Xvalid, ytrain, yvalid = train_test_split(X_train_imp_norm[tr_idx], Y_train, test_size = 0.3, random_state = 0)
  print("K = {}".format(K_it))
  R = Regression_model(Xtrain, ytrain)
  R.Lasso_reg([1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])
  Score = R.score_dataset(Xvalid, yvalid)
  MAE_array_valid.append(Score[0])
  R2_array_valid.append(Score[1])
  RMSE_array_valid.append(Score[2])
  axs[0].plot(K_it, Score[0],"o", label ="K ={}".format(K_it))
  axs[1].plot(K_it, Score[1],"o", label ="K ={}".format(K_it))
  axs[2].plot(K_it, Score[2],"o", label ="K ={}".format(K_it))
axs[0].set_ylabel("MAE")
axs[1].set_ylabel("R2")
axs[2].set_ylabel("RMSE")
axs[0].set_xlabel("Regression method")
axs[1].set_xlabel("Regression method")
axs[2].set_xlabel("Regression method")
fig.suptitle('Regression methods Performences - Lasso')
plt.legend()
plt.show()

"""*Only float Variabels using KNN:*"""

fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=False)
for K_it in np.linspace(2,50,25, dtype= int):
  X = initialize_data(train, test, categorical_cols)
  I = Impute_class(X)
  I.impute_values(cols=categorical_cols, method= "most_frequent")
  I.impute_values(cols=int_cols, method= "mean")
  I.impute_values(cols=float_cols, method= "KNN",K= K_it)
  X = I.X
  X_train_imp_norm =  pd.DataFrame(stats.zscore(X))
  X_train_imp_norm.columns = X.columns
  Xtrain, Xvalid, ytrain, yvalid = train_test_split(X_train_imp_norm[tr_idx], Y_train, test_size = 0.3, random_state = 0)
  print("K = {}".format(K_it))
  R = Regression_model(Xtrain, ytrain)
  R.Lasso_reg([1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])
  Score = R.score_dataset(Xvalid, yvalid)
  MAE_array_valid.append(Score[0])
  R2_array_valid.append(Score[1])
  RMSE_array_valid.append(Score[2])
  axs[0].plot(K_it, Score[0],"o", label ="K ={}".format(K_it))
  axs[1].plot(K_it, Score[1],"o", label ="K ={}".format(K_it))
  axs[2].plot(K_it, Score[2],"o", label ="K ={}".format(K_it))
axs[0].set_ylabel("MAE")
axs[1].set_ylabel("R2")
axs[2].set_ylabel("RMSE")
axs[0].set_xlabel("Regression method")
axs[1].set_xlabel("Regression method")
axs[2].set_xlabel("Regression method")
fig.suptitle('Regression methods Performences - Lasso')
plt.legend()
plt.show()

fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=False)
for K_it in np.linspace(2,50,25, dtype= int):
  X = initialize_data(train, test, categorical_cols)
  I = Impute_class(X)
  I.impute_values(cols=categorical_cols, method= "KNN",K= K_it)
  I.impute_values(cols=int_cols, method= "KNN",K= K_it)
  I.impute_values(cols=float_cols, method= "KNN",K= K_it)
  X = I.X
  X_train_imp_norm =  pd.DataFrame(stats.zscore(X))
  X_train_imp_norm.columns = X.columns
  Xtrain, Xvalid, ytrain, yvalid = train_test_split(X_train_imp_norm[tr_idx], Y_train, test_size = 0.3, random_state = 0)
  print("K = {}".format(K_it))
  R = Regression_model(Xtrain, ytrain)
  R.Lasso_reg([1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])
  Score = R.score_dataset(Xvalid, yvalid)
  MAE_array_valid.append(Score[0])
  R2_array_valid.append(Score[1])
  RMSE_array_valid.append(Score[2])
  axs[0].plot(K_it, Score[0],"o", label ="K ={}".format(K_it))
  axs[1].plot(K_it, Score[1],"o", label ="K ={}".format(K_it))
  axs[2].plot(K_it, Score[2],"o", label ="K ={}".format(K_it))
axs[0].set_ylabel("MAE")
axs[1].set_ylabel("R2")
axs[2].set_ylabel("RMSE")
axs[0].set_xlabel("Regression method")
axs[1].set_xlabel("Regression method")
axs[2].set_xlabel("Regression method")
fig.suptitle('Regression methods Performences - Lasso')
plt.legend()
plt.show()

"""**Categorical Value:**

MAE = 0.09198478447624521
R2 = 0.7993810645978225
RMSE = 0.17606104103589937


**Int Values:**

MAE = 0.09198478447624521
R2 = 0.7993810645978225
RMSE = 0.17606104103589937

**float Values:**

K = 8
MAE = 0.09194339972473553
R2 = 0.7992661154371496
RMSE = 0.17611147289249104

K = 10
MAE = 0.09197296137523692
R2 = 0.7997209415512774
RMSE = 0.17591184162060375

**all KNN:**

K = 8
MAE = 0.09194339972473553
R2 = 0.7992661154371496
RMSE = 0.17611147289249104

K = 10
MAE = 0.09197296137523692
R2 = 0.7997209415512774
RMSE = 0.17591184162060375



**We can notice that applying KNN imputing is beneficail only when applied on float represented Variabels, and performs better then not using KNN at all *
"""

house_df.iloc[705]

"""### 3.2.2 Label Encoding vs Factorization """

# Using Label Encoding in order to mess with Categorical Values
fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=False)
MAE_array_train = []
MAE_array_valid = []
R2_array_train = []
R2_array_valid = []
RMSE_array_train = []
RMSE_array_valid = []
for K_it in np.linspace(2,50,25, dtype= int):
  X, Y_train, X_test, Y_test = initialize_data(train, test, categorical_cols)
  I = Impute_class(X)
  I.impute_values(cols=categorical_cols, method= "most_frequent")
  I.impute_values(cols= int_cols, method= "median")
  I.impute_values(cols= float_cols, method= "KNN", K=K_it)
  X = I.X
  X_train_imp_norm =  pd.DataFrame(stats.zscore(X))
  X_train_imp_norm.columns = X.columns
  Xtrain, Xvalid, ytrain, yvalid = train_test_split(X_train_imp_norm[tr_idx], Y_train, test_size = 0.3, random_state = 0)
  print("K = {}".format(K_it))
  R = Regression_model(Xtrain, ytrain)
  R.Lasso_reg([1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])
  Score = R.score_dataset(Xvalid, yvalid)
  MAE_array_valid.append(Score[0])
  R2_array_valid.append(Score[1])
  RMSE_array_valid.append(Score[2])
  axs[0].plot(K_it, Score[0],"o", label ="K ={}".format(K_it))
  axs[1].plot(K_it, Score[1],"o", label ="K ={}".format(K_it))
  axs[2].plot(K_it, Score[2],"o", label ="K ={}".format(K_it))

  # Score = R.score_dataset(X_test, Y_test)
axs[0].set_ylabel("MAE")
axs[1].set_ylabel("R2")
axs[2].set_ylabel("RMSE")
axs[0].set_xlabel("Regression method")
axs[1].set_xlabel("Regression method")
axs[2].set_xlabel("Regression method")
fig.suptitle('Regression methods Performences - Lasso')
plt.legend()
plt.show()

"""Similarly, Best performences given by setting K =10.

The main difference between Factorize and label encoding is that label encoding sets the Major class in each categorical value to be the highest numerical value and the minor to be 0, in contrast to factorization which does the opposite!

Factorize Encoding Results:

MAE = 0.09197296137523692
R2 = 0.7997209415512774
RMSE = 0.17591184162060375

Label Encoding Results:

MAE = 0.09113030833080774
R2 = 0.8066921745407033
RMSE = 0.17282319211095384

### Adding Outliers removal
"""

# Using Label Encoding in order to mess with Categorical Values
X_train, Y_train, X_test, Y_test, categorical_cols, tr_idx, te_idx = initialize_data()
# print(X)
I = Impute_class(X_train)
I.impute_values(cols=categorical_cols, method= "most_frequent")
I.impute_values(cols= int_cols, method= "median")
I.impute_values(cols= float_cols, method= "KNN", K=10)
X_train = I.X
Xtrain, Xvalid, ytrain, yvalid = train_test_split(X_train, Y_train, test_size = 0.3, random_state = 0)
print("X_train", X_train.shape)
print("Y_train", Y_train.shape)
print("X_test", Xvalid.shape)
print("Y_test", yvalid.shape)
R = Regression_model(Xtrain, ytrain)
R.Lasso_reg([1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])
Score = R.score_dataset(Xvalid, yvalid)

ytrain = pd.Series(ytrain)
ytrain.isna()

# Using Label Encoding in order to mess with Categorical Values
X_train, Y_train, X_test, Y_test, categorical_cols, tr_idx, te_idx = initialize_data()
# print(X)
I = Impute_class(X_train)
I.impute_values(cols=categorical_cols, method= "most_frequent")
I.impute_values(cols= int_cols, method= "median")
I.impute_values(cols= float_cols, method= "KNN", K=10)
X_train = I.X
Xtrain, Xvalid, ytrain, yvalid = train_test_split(X_train, Y_train, test_size = 0.3, random_state = 0)
print("X_train", X_train.shape)
print("Y_train", Y_train.shape)
print("X_test", Xvalid.shape)
print("Y_test", yvalid.shape)
R = Regression_model(Xtrain, ytrain)
R.Lasso_reg([1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])
Score = R.score_dataset(Xvalid, yvalid)

# Using Label Encoding in order to mess with Categorical Values
fig, axs = plt.subplots(1, 3, figsize=(36, 12), sharey=False)
MAE_array_train = []
MAE_array_valid = []
R2_array_train = []
R2_array_valid = []
RMSE_array_train = []
RMSE_array_valid = []
for K_it in np.linspace(2,50,25, dtype= int):

  X_train, Y_train, X_test, Y_test, categorical_cols, tr_idx, te_idx = initialize_data()
  # print(X)
  I = Impute_class(X_train)
  I.impute_values(cols=categorical_cols, method= "most_frequent")
  I.impute_values(cols= int_cols, method= "median")
  I.impute_values(cols= float_cols, method= "KNN", K=K_it)
  X_train = I.X
  X_train =  pd.DataFrame(stats.zscore(X_train))
  X_train.columns = X_train.columns
  Xtrain, Xvalid, ytrain, yvalid = train_test_split(X_train, Y_train, test_size = 0.3, random_state = 0)
  print("K = {}".format(K_it))
  R = Regression_model(Xtrain, ytrain)
  R.Lasso_reg([1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])
  Score = R.score_dataset(Xvalid, yvalid)
  MAE_array_valid.append(Score[0])
  R2_array_valid.append(Score[1])
  RMSE_array_valid.append(Score[2])
  axs[0].plot(K_it, Score[0],"o", label ="K ={}".format(K_it))
  axs[1].plot(K_it, Score[1],"o", label ="K ={}".format(K_it))
  axs[2].plot(K_it, Score[2],"o", label ="K ={}".format(K_it))

  # Score = R.score_dataset(X_test, Y_test)
axs[0].set_ylabel("MAE")
axs[1].set_ylabel("R2")
axs[2].set_ylabel("RMSE")
axs[0].set_xlabel("Regression method")
axs[1].set_xlabel("Regression method")
axs[2].set_xlabel("Regression method")
fig.suptitle('Regression methods Performences - Lasso')
plt.legend()
plt.show()

"""After Outliers removal, best result given cy choosing K = 4

K = 4

MAE = 0.08874380358603727
R2 = 0.9073959587847078
RMSE = 0.12353716219630897
"""

from scipy import stats
# Using Label Encoding in order to mess with Categorical Values
X_train, Y_train, X_test, Y_test, categorical_cols, tr_idx, te_idx = initialize_data()
# print(X)
I = Impute_class(X_train)
I.impute_values(cols=categorical_cols, method= "most_frequent")
I.impute_values(cols= int_cols, method= "median")
I.impute_values(cols= float_cols, method= "KNN", K=4)
X_train = I.X
X_train = pd.DataFrame(stats.zscore(X_train))
X_train.columns = X_train.columns
Xtrain, Xvalid, ytrain, yvalid = train_test_split(X_train, Y_train, test_size = 0.3, random_state = 0)
print("X_train", X_train.shape)
print("Y_train", Y_train.shape)
print("X_test", Xvalid.shape)
print("Y_test", yvalid.shape)
R = Regression_model(Xtrain, ytrain)
R.Lasso_reg([1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])
Score = R.score_dataset(Xvalid, yvalid)
Score = R.score_dataset(Xvalid, yvalid)