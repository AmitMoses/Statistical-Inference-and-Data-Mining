# -*- coding: utf-8 -*-
"""House_Prices_edited_dor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J3snX9fg76spFXfk0G5Idj9h9s4Bgpbj

#House Prices - Advanced Regression Techniques
##Predict sales prices and practice feature engineering, RFs, and gradient boosting

import libraries and data loading from Kaggle "house-prices-advanced-regression-techniques"
and loading data
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

from google.colab import drive
drive.mount('/content/gdrive')
# main_path = '/content/gdrive/Othercomputers/ה-מחשב נייד שלי/תואר שני/הסקה סטטיסטית וכריית נתונים/Assignments/HW2/Kaggle - House Prices/house-prices-advanced-regression-techniques'
main_path = '/content/gdrive/MyDrive/Statistical_Data_Mining/Assignment_2/house-prices-advanced-regression-techniques'

train = pd.read_csv(main_path + '/train.csv')
test = pd.read_csv(main_path + '/test.csv')
submit = pd.read_csv(main_path + "/sample_submission.csv")

"""# Section 1: Database Overview and Visualization

In this section we will overview the database, understand it structure.

## 1.1 Data discription

We will make new database 'house_df' that is the concatinate of the test and train so wi can remove or add features to all the frames conveniently
"""

print("train database:")
train

print("test database:")
test

house_df = pd.concat([train,test],ignore_index = True, sort = False)
tr_idx = house_df['SalePrice'].notnull() ## display data with on
te_idx = [not elem for elem in tr_idx]
te_idx = pd.Series(te_idx)
print("house dataframe shape:{}".format(house_df.shape))

"""## 1.2 Missing featurs

There are X fearute that are not existe in all the samples in the dataframe:
* LotFrontage
* Alley
* BsmtQual
* MasVnrArea
* MasVnrArea
* BsmtCond
* BsmtExposure
* BsmtFinType1
* BsmtFinType2
* Electrical
* FireplaceQu     
* GarageType                                                           
* GarageCond       
* PoolQC         
* Fence          
* MiscFeature

Train and Test missing data:
"""

# house_df missing features
print("house_df missing features")
print("house_df shape: {}".format(train.shape))
pd.set_option('display.max_rows', None)
missing_data = pd.DataFrame(house_df.isna().sum())
missing_data_prec = missing_data/house_df.shape[0] * 100
missing_data = pd.concat([missing_data,missing_data_prec], axis=1)
missing_data.columns =['occurence', 'precentage']
print(missing_data)
pd.reset_option('all')

"""Train missing data:"""

# train missing features
print('train missing features')
print('train shape: {}'.format(train.shape[0]))
pd.set_option('display.max_rows', None)
missing_data = pd.DataFrame(train.isna().sum())
missing_data_prec = missing_data/train.shape[0] * 100
missing_data = pd.concat([missing_data,missing_data_prec], axis=1)
missing_data.columns =['occurence', 'precentage']
print(missing_data)
pd.reset_option('all')

"""***Most missing features are:***
1.   "Alley" with 93% of missing values
2.   "PoolQC" with 99.52% of missing values
3.   "Fence" with 80.75% of missing values
4.   "MiscFeature" with 96.3% of missing values

## 1.3 Features type distribution

There are 81 diffrent feature, that is a lot of data to handle and process. Additionally, the feature are in diffrent type, not all of them are numerical, they can be splited according to type: 
1. int - 43 features
2. float64 - 25 features
3. object - 12 features
"""

plt.figure(figsize = (8,6))
ax = house_df.dtypes.value_counts().plot(kind='bar',grid = False,fontsize=20,color='grey')
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x()+ p.get_width() / 2., height + 1, height, ha = 'center', size = 25)
sns.despine()

"""Statistical description of the numerical features"""

house_df.describe().T.style.set_properties(**{'background-color': 'Grey',
                           'color': 'white',
                           'border-color': 'darkblack'})

"""## 1.4 Feature Histogram

Spliting the data acording to the type from the previuse subsection and ploting hitsograme for every feature to further understanding the numerical meaning behind the features.
It can be conclude from the histogram that:

1. **float64 features:**

  Three float64 features are in fact discrete variables:
  * BsmtHalfBath - Basement half bathrooms
  * BsmtFullBath - Basement full bathrooms
  * GarageCars - Size of garage in car capacity
  
  Some features have a skewed shape to one side.


2. **int features:**

  Five int features values does not represent quantitative sizes
  * MSSubClass - Identifies the type of dwelling involved in the sale.
  * YearBuilt - Original construction date
  * YearRemodAdd - Remodel date (same as construction date if no remodeling or additions)
  * MoSold - Month Sold (MM)
  * YrSold - Year Sold (YYYY)


"""

# Select categorical columns
categorical_cols = [cname for cname in house_df.loc[:,:'SaleCondition'].columns if
                    house_df[cname].nunique() < 200 and 
                    house_df[cname].dtype == "object"]

print(len(categorical_cols))
# Select numerical columns
int_cols = [cname for cname in house_df.loc[:,:'SaleCondition'].columns if 
                house_df[cname].dtype in ['int64']]
float_cols = [cname for cname in house_df.loc[:,:'SaleCondition'].columns if 
                house_df[cname].dtype in ['float64']]

print("int type features:")
plt.figure(figsize=(20, 20))
plt.subplots_adjust(hspace=0.5)
for i, var in enumerate(int_cols):
    plt.subplot(6,5,i+1)
    fig = train[var].hist(bins=60)
    fig.set_ylabel('number of houses')
    fig.set_xlabel(var)
    str3= "".join([var,' Histogram'])
    fig.set_title(str3)

print("float64 type features:")
plt.figure(figsize=(20, 20))
plt.subplots_adjust(hspace=1)
for i, var in enumerate(float_cols):
    plt.subplot(4,4,i+1)
    fig = train[var].hist(bins=60)
    fig.set_ylabel('number of houses')
    fig.set_xlabel(var)
    str3= "".join([var,' Histogram'])
    fig.set_title(str3)

print("Categorial type features:")
plt.figure(figsize=(30, 30))
plt.subplots_adjust(hspace=0.5)
for i, var in enumerate(categorical_cols):
    plt.subplot(11,4,i+1)
    fig = train[var].hist(bins=60)
    fig.set_ylabel('number of houses')
    fig.set_xlabel(var)
    str3= "".join([var,' Histogram'])
    fig.set_title(str3)

"""factorize categorial values

# Section 2: Feature Engineering

## 2.1 facturize categorical features
"""

for c in categorical_cols:
  # print(house_df[c])
  house_df[c] = pd.factorize(house_df[c])[0]
  # print(house_df[c])
  # print(np.array(new_c).shape)

print("Categorial type features:")
plt.figure(figsize=(30, 30))
plt.subplots_adjust(hspace=0.5)
for i, var in enumerate(categorical_cols):
    plt.subplot(11,4,i+1)
    fig = house_df[tr_idx][var].hist(bins=60)
    fig.set_ylabel('number of houses')
    fig.set_xlabel(var)
    str3= "".join([var,' Histogram'])
    fig.set_title(str3)

"""## 2.2 Taking care of skewed target

The Sale price is a bit skewed, and for linear models its important that you normalize your data, so im going to log transform it
"""

plt.figure()
sns.distplot(house_df['SalePrice']);
house_df['SalePrice'] = np.log1p(house_df['SalePrice'])
plt.title('Target SalePrice - skewed')
plt.figure()
sns.distplot(house_df['SalePrice'])
plt.title('Log Transform Target SalePrice - unskewed')

"""## 2.3 Missing Values

https://scikit-learn.org/stable/auto_examples/impute/plot_missing_values.html#sphx-glr-auto-examples-impute-plot-missing-values-py
"""

from sklearn.experimental import enable_iterative_imputer  # noqa
from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer

X_train = house_df[tr_idx].drop(['SalePrice'], axis=1)
X_test = house_df[te_idx]
Y_train = house_df[tr_idx].SalePrice
Y_test = house_df[te_idx].SalePrice
overall_X = house_df.drop(['SalePrice'], axis=1)
print(X_train.shape)
print(Y_train.shape)
print(X_test.shape)
print(Y_test.shape)
print(overall_X.shape)

"""### option 1 - get rid of the columns with the missing values

usually not the best solution. However, it can be useful when most values in a column are missing
"""

missing_data = [col for col in X_train.columns if (X_train[col].isnull().sum() / X_train.shape[0]) * 100 > 50]
print('missing_data: {}'.format(missing_data))
X_df_drop = overall_X.drop(missing_data, axis=1)
X_train_drop = X_df_drop[tr_idx]
X_test_drop = X_df_drop[te_idx]

print('X_df_drop shape:     {}'.format(X_df_drop.shape))
print('X_train_drop shape:  {}'.format(X_train_drop.shape))
print('X_test_drop shape:   {}'.format(X_test_drop.shape))
print(type(tr_idx))

"""### option 2 - replacing with statistical value

we should try to apply the missing values algorithms for the concatanate data, and on the train data by itself.

from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer
class Imputer(object):
  def __init__(self, X_train, X_test):
    self.X_train = X_train
    self.X_test = X_test
    self.X = pd.concat([X_train, X_test], ignore_index = True, sort = False)

  def zero_imputer(self):
    imputer = SimpleImputer(add_indicator=True, strategy="constant", fill_value=0)
    imputer.fit(self.X)
    new_X = imputer.transform(X)
    new_x_train = imputer.transform(self.X_train)
    nex_X_test = imputer.transform(self.X_test)

  def mean_imputer(self):
    imputer = SimpleImputer(add_indicator=True, strategy="mean")
    new_X = imputer.transform(X)
    new_x_train = imputer.transform(self.X_train)
    nex_X_test = imputer.transform(self.X_test)

  def median_imputer(self):
    imputer = SimpleImputer(add_indicator=True, strategy="median")
    new_X = imputer.transform(X)
    new_x_train = imputer.transform(self.X_train)
    nex_X_test = imputer.transform(self.X_test)

  def most_fre_imputer(self):
    imputer = SimpleImputer(add_indicator=True, strategy="most_frequent")
    new_X = imputer.transform(X)
    new_x_train = imputer.transform(self.X_train)
    nex_X_test = imputer.transform(self.X_test)

  def KNN_imputer(self, K):
    imputer = KNNImputer(n_neighbors=K, weights="uniform")
    new_X = imputer.transform(X)
    new_x_train = imputer.transform(self.X_train)
    nex_X_test = imputer.transform(self.X_test)
"""

from sklearn.impute import SimpleImputer

def Imputer(strategy_method, dataset, col ):
  fill_NaN = SimpleImputer(add_indicator=False, strategy=strategy_method)
  imputed_DF = pd.DataFrame(fill_NaN.fit_transform(dataset[col]))
  imputed_DF.columns = dataset[col].columns

  dataset[col] = imputed_DF
  return dataset

# filling  missing values in most_frequent items
X_train_imp = Imputer("most_frequent",X_df_drop, categorical_cols )
X_train_imp = Imputer("median",X_df_drop, int_cols)
X_train_imp = Imputer("mean",X_df_drop, float_cols)
X_train_imp

"""##2.4 Normalize features

https://www.width.ai/pandas/normalize-column-pandas-dataframe
"""

from scipy import stats
X_train_imp_norm =  pd.DataFrame(stats.zscore(X_train_imp))
X_train_imp_norm.columns = X_df_drop.columns
X_train_imp_norm

"""from sklearn.cluster import KMeans
kmeans = KMeans(3,init='k-means++')
kmeans.fit(df_preprocessed.drop('species',axis=1))
wcss=[]
for i in range(1,10):
    kmeans = KMeans(i)
    kmeans.fit(df_preprocessed.drop('species',axis=1))
    pred_i = kmeans.labels_
    wcss.append(kmeans.inertia_)

plt.figure(figsize=(10,6))
plt.plot(range(1,10),wcss)
plt.ylim([0,1800])
plt.title('The Elbow Method',{'fontsize':20})
plt.xlabel('Number of clusters')
plt.ylabel('Within-cluster Sum of Squares');

new_X_train

def optimize_k(data, target):
    errors = []
    for k in range(1, 20, 2):
        imputer = KNNImputer(n_neighbors=k)
        imputed = imputer.fit_transform(data)
        df_imputed = pd.DataFrame(imputed, columns=df.columns)
        
        X = df_imputed.drop(target, axis=1)
        y = df_imputed[target]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        model = RandomForestRegressor()
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        error = rmse(y_test, preds)
        errors.append({'K': k, 'RMSE': error})
        
    return errors
k_errors = optimize_k(data=df, target='MEDV')

## 2.5 Corrulations

Relationship between SalePrice and other numerical columns
"""

#correlation matrix
corrmat = train.corr()
f, ax = plt.subplots(figsize=(16, 14))
sns.heatmap(corrmat, vmax=.8, square=True);

"""# Section 3: Implement prediction models

## 3.1 Model implemention
"""

from sklearn.linear_model import LinearRegression
from sklearn.kernel_ridge import KernelRidge
from sklearn.linear_model import Ridge
from sklearn.linear_model import RidgeCV
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score
"""
class Regression_model(object):
    def __init__(self, X, Y):
        self.X = X
        self.Y = Y

    def Linear_reg(self):
        self.model = LinearRegression(normalize=True)
        self.model.fit(self.X, self.Y)
        
    def Ridge_reg(self, alpha, X, Y):
        self.model = Ridge(alpha)
        self.model.fit(X, Y)

    def KernelRidge_reg(self, alpha):
        self.model = KernelRidge(alpha)
        self.model.fit(self.X, self.Y)

    def RidgeKfold_reg(self, alphas):
        # should enter a list of alphas - algorithem uses the most beneficial
        # alphas = [1e-4, 1e-3, 1e-2, 1e-1, 1] 
        self.model = RidgeCV(alphas)
        self.model.fit(self.X, self.Y)

    def Poly_reg(self):
        self.model = PolynomialFeatures(interaction_only=True)
        self.model.fit(self.X, self.Y)

    def score_dataset(self, X_test, y_test):
        preds = self.model.predict(X_test)
        return mean_absolute_error(y_test, preds), accuracy_score(y_test,preds), self.model.score(X_test, y_test)
"""
def Regression_model(method,train_data, test_data,alpha):
  if method == "Linear_reg":
    print('Linear_reg')
    model = LinearRegression()
  elif method == "Ridge_reg":
    print('Ridge_reg')
    model = Ridge(alpha)
  elif method == "KernelRidge_reg":
    print('KernelRidge_reg')
    model = KernelRidge(alpha)
  else:
    print('none')
  
  model.fit(train_data,test_data)
  return model

"""## 3.2 Data Split

Splitting the original train data into train and validation 

consider use k-fold
"""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
print(X_train_imp_norm.shape)
print(Y_train.shape)
Xtrain, Xvalid, ytrain, yvalid = train_test_split(X_train_imp[tr_idx], Y_train, test_size = 0.3, random_state = 0)
print("Xtrain : " + str(Xtrain.shape))
print("Xtest : " + str(Xtest.shape))
print("ytrain : " + str(ytrain.shape))
print("ytest : " + str(ytest.shape))

"""https://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/

## 3.2 Evaluate models
"""

def rmse_cv(model, df, target):
    rmse = np.sqrt(-cross_val_score(model, df, target, scoring = "neg_mean_squared_error"))
    return(rmse)

def r2_cv(model, df, target):
    r2 = cross_val_score(model, df, target, scoring = 'r2')
    return(r2)

def evaluate_model(method, Xtrain, ytrain, alpha):
  model = Regression_model(method, Xtrain, ytrain,alpha)
  print("RMSE on Training set : ", rmse_cv(model, Xtrain, ytrain).mean())
  print("R2 on Training set :   ", r2_cv(model, Xtrain, ytrain).mean())
  print("RMSE on Valid set :    ", rmse_cv(model, Xvalid, yvalid).mean())
  print("R2 on Valid set :      ", r2_cv(model, Xvalid, yvalid).mean())
  print()
  return model

Linear_reg_model = evaluate_model("Linear_reg", Xtrain, ytrain, alpha='NaN')
Ridge_reg_model = evaluate_model("Ridge_reg", Xtrain, ytrain, alpha=10)
KernelRidge_reg_model = evaluate_model("KernelRidge_reg", Xtrain, ytrain, alpha=1)